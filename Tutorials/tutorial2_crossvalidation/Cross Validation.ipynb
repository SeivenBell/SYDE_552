{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(this notebook is adapted by kevin zhu for SYDE 522. The original notebook is from https://github.com/vaasha/Machine-leaning-in-examples/blob/master/sklearn/cross-validation/Cross%20Validation.ipynb)\n",
    "\n",
    "# Cross-validation\n",
    "(from Wikipedia)\n",
    "\n",
    ">In a prediction problem, a model is usually given a dataset of known data on which **training** is run (training dataset), and a dataset of unknown data (or first seen data) against which the model is **tested** (called the validation dataset or testing set). \n",
    ">\n",
    ">The goal of **cross-validation** is to test the model's ability to predict new data that was not used in estimating it, in order to flag problems like overfitting or selection bias and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem).\n",
    "\n",
    "### K-fold cross-validation\n",
    "(still from Wikipedia)\n",
    ">In *k*-fold cross-validation, the original sample is randomly partitioned into *k* equal sized subsamples, often referred to as \"folds\". Of the *k* subsamples, a single subsample is retained as the validation data for testing the model, and the remaining *k âˆ’ 1* subsamples are used as training data. The cross-validation process is then repeated *k* times, with each of the *k* subsamples used exactly once as the validation data. The *k* results can then be averaged to produce a single estimation. \n",
    ">\n",
    ">The advantage of this method over repeated random sub-sampling is that all observations are used for both training and validation, and each observation is used for validation exactly once. \n",
    "\n",
    "More: https://en.wikipedia.org/wiki/Cross-validation_(statistics)\n",
    "\n",
    "![](Cross_Validation.jpg \"Cross validation folds\")\n",
    "\n",
    "### Implementation\n",
    "In this notebook, we will explore the `Scikit-learn` library's implementation of cross-validation in python.\n",
    "Their official documentation is here: https://scikit-learn.org/stable/modules/cross_validation.html.\n",
    "\n",
    "`Scikit-learn` is a popular open source library that offers tools for predictive data analysis. More:\n",
    "https://scikit-learn.org/stable/getting_started.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you haven't already, install the required libraries\n",
    "\n",
    "# !pip install numpy pandas matplotlib seaborn scikit-learn\n",
    "\n",
    "# or from the requirements file:\n",
    "\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split, cross_validate, cross_val_score\n",
    "from sklearn.datasets import load_iris \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from utils import plot_kfold\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using sklearn, let's implement k-fold cross-validation from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Performs vanilla K-fold split on a dataset of length N.\n",
    "Returns the indices of the training and test set for each fold.\n",
    "\n",
    "Args:\n",
    "    K (int): number of folds.\n",
    "    N (int): total number of observations in dataset.\n",
    "\n",
    "Returns:\n",
    "    A list of K tuples of size 2: \n",
    "\n",
    "        [ (train_idx_1, test_idx_1),\n",
    "          (train_idx_2, test_idx_2), \n",
    "          ..., \n",
    "          (train_idx_K, test_idx_K) ]\n",
    "\n",
    "        where:\n",
    "            train_idx_k (list): indices of training data for the kth fold.\n",
    "            test_idx_k (list): indices of test data for the kth fold.\n",
    "\n",
    "        \n",
    "\n",
    "Example:\n",
    "    vanilla_kfold(K=3, N=6) should return:\n",
    "\n",
    "    [([2, 3, 4, 5], [0, 1]), \n",
    "     ([0, 1, 4, 5], [2, 3]), \n",
    "     ([0, 1, 2, 3], [4, 5])]\n",
    "\n",
    "Note: all observations should appear once in the test set. Example:\n",
    "    vanilla_kfold(K=3, N=7) should return:\n",
    "    \n",
    "    [([2, 3, 4, 5, 6],  [0, 1]),\n",
    "     ([0, 1, 4, 5, 6],  [2, 3]),\n",
    "     ([0, 1, 2, 3],     [4, 5, 6])]\n",
    "    \n",
    "    or [([3, 4, 5, 6],     [0, 1, 2]),\n",
    "        ([0, 1, 2, 5, 6],  [3, 4]),\n",
    "        ([0, 1, 2, 3, 4],  [5, 6])]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def vanilla_kfold(K, N):\n",
    " \n",
    "    testsize = N//K\n",
    "    idx = set(range(N))\n",
    " \n",
    "    folds = []\n",
    "    for k in range(K):\n",
    "        start = k* testsize\n",
    "        end = (k+1)* testsize if k < (K-1) else N\n",
    "        \n",
    "        test_idx = set(range(start, end))\n",
    "        train_idx = idx - test_idx\n",
    " \n",
    "        fold = (list(train_idx), list(test_idx))\n",
    "        folds.append(fold)\n",
    " \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split some fake data of size 25 into 5 folds using our implementation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 5-fold:\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Your Code Goes Here",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# plot the indices\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor 5-fold:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (train_index, test_index) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mvanilla_kfold\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_fake\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFold \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: Train idx \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m;   Test idx \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, train_index, test_index))\n",
      "Cell \u001b[1;32mIn[3], line 45\u001b[0m, in \u001b[0;36mvanilla_kfold\u001b[1;34m(K, N)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvanilla_kfold\u001b[39m(K, N):\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour Code Goes Here\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Your Code Goes Here"
     ]
    }
   ],
   "source": [
    "# create some fake data of size 25\n",
    "N_fake = 25\n",
    "fakedata = np.arange(N_fake)\n",
    "\n",
    "# plot the indices\n",
    "print(\"for 5-fold:\")\n",
    "for i, (train_index, test_index) in enumerate(vanilla_kfold(5, N_fake)):\n",
    "    print(\"Fold {}: Train idx {};   Test idx {}\".format(i + 1, train_index, test_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot results for 5- and 3-folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2, figsize=(12, 4))\n",
    "\n",
    "ax = plt.subplot(121)\n",
    "plot_kfold(ax, vanilla_kfold(5, N_fake), N_fake)\n",
    "ax.set_title(\"5 Folds\")\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "plot_kfold(ax, vanilla_kfold(3, N_fake), N_fake)\n",
    "plt.yticks(np.arange(1, 4, 1))\n",
    "ax.set_title(\"3 Folds\")\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare to sklearn's implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf5 = KFold(n_splits=5, shuffle=False)  # 5-fold\n",
    "kf3 = KFold(n_splits=3, shuffle=False)  # 3-fold\n",
    "\n",
    "# print indices for 5-fold:\n",
    "print(\"for 5-fold:\")\n",
    "for i, (train_index, test_index) in enumerate(kf5.split(fakedata)):\n",
    "    print(\"Fold {}: Train idx {};   Test idx {}\".format(i + 1, train_index, test_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2, figsize=(12, 4))\n",
    "\n",
    "ax = plt.subplot(121)\n",
    "plot_kfold(ax, kf5.split(fakedata), N_fake)\n",
    "ax.set_title(\"5 Folds\")\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "plot_kfold(ax, kf3.split(fakedata), N_fake)\n",
    "plt.yticks(np.arange(1, 4, 1))\n",
    "ax.set_title(\"3 Folds\")\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffled K-fold\n",
    "We can also shuffle the test indices for each fold by setting `shuffle = True`. Each observation should still appear in the test set once after all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffled 5-fold with different random states\n",
    "kf5_shuffled42 = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "kf5_shuffled123 = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "# ====================================================================================\n",
    "# plot\n",
    "plt.figure(2, figsize=(12, 4))\n",
    "\n",
    "ax = plt.subplot(121)\n",
    "plot_kfold(ax, kf5_shuffled42.split(fakedata), N_fake)\n",
    "ax.set_title(\"Shuffled 5-Fold with random state 42\")\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "plot_kfold(ax, kf5_shuffled123.split(fakedata), N_fake)\n",
    "ax.set_title(\"Shuffled 5-Fold with random state 123\")\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold on the Iris dataset\n",
    "\n",
    "Let's use K-fold to evaluate a classification model on the popular Iris Dataset. \n",
    "\n",
    "The Iris dataset contains 150 samples of petal and sepal sizes for 3 species of irises' - setosa, versicolor and virginica. More: https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris(return_X_y=False)  # load Iris dataset from sklearn\n",
    "\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "\n",
    "features = iris[\"feature_names\"]\n",
    "iris_df[\"target\"] = iris.target\n",
    "iris_df[\"target_name\"] = iris_df[\"target\"].map(\n",
    "    {i: name for i, name in enumerate(iris.target_names)}\n",
    ")\n",
    "\n",
    "print(\"Data overview:\")\n",
    "iris_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The occurrence of the classes are evenly distributed in the dataset. There are 50 samples of each species (1/3 of entire data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the occurrence of each class\n",
    "pd.DataFrame(iris_df.groupby(\"target_name\").size().reset_index()).rename(\n",
    "    columns={0: \"sample size\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without K-fold\n",
    "\n",
    "Let's start without K-fold.\n",
    "\n",
    "We split the data randomly into training and test sets using `train_test_split`.\n",
    "\n",
    "We fit a logistics regression model on the training set to classify the Iris species from petal and sepal sizes. We evaluate our model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into 80% train and 20% test set (no kfold)\n",
    "X = iris_df[features]\n",
    "y = iris_df[\"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# initialize the model (logistic regression)\n",
    "model = LogisticRegression(solver=\"liblinear\", multi_class=\"auto\")\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train)  # fit the model\n",
    "\n",
    "# evaluate results:\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Accuracy on Train\": [accuracy_score(y_train, model.predict(X_train))],\n",
    "        \"Accuracy on Test\": [accuracy_score(y_test, model.predict(X_test))],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold (K=3)\n",
    "\n",
    "Now, let's evaluate using 3-fold cross-validation.\n",
    "\n",
    "For each fold, we fit a logistic regression model on the training set and evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (train_index, test_index) in enumerate(\n",
    "    kf3.split(iris_df)\n",
    "):  # train and evaluate for each fold\n",
    "    X_train = iris_df.iloc[train_index].loc[:, features]\n",
    "    X_test = iris_df.iloc[test_index][features]\n",
    "    y_train = iris_df.iloc[train_index].loc[:, \"target\"]\n",
    "    y_test = iris_df.loc[test_index][\"target\"]\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)  # Training the model\n",
    "    print(\n",
    "        f\"Test accuracy for fold {i+1}: {accuracy_score(y_test, model.predict(X_test))}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the test accuracy **0** for every fold? \n",
    "\n",
    "In each fold, the model never sees one of the species during training, while the test set contains exclusively that unseen species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = iris_df[\"target\"]\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(13, 4), sharey=True)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf3.split(iris_df)):\n",
    "    ax[i].scatter(x=train_index, y=target_name.iloc[train_index], label=\"train\")\n",
    "    ax[i].scatter(x=test_index, y=target_name.iloc[test_index], label=\"test\")\n",
    "    ax[i].set_title(f\"Fold {i+1}\")\n",
    "    ax[i].set_xlabel(\"indices\")\n",
    "\n",
    "ax[0].set_yticks([0, 1, 2])\n",
    "ax[0].set_yticklabels(iris[\"target_names\"])\n",
    "ax[0].legend()\n",
    "\n",
    "plt.suptitle(\"Vanilla K-Fold\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we want all training and test sets to contain samples of all 3 species."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffled K-fold\n",
    "\n",
    "One way to get around this issue is using shuffled K-fold to randomly sample the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=3, shuffle=True, random_state=123)  # shuffled 3-fold\n",
    "\n",
    "# ====================================================================================\n",
    "fig, ax = plt.subplots(1, 3, figsize=(13, 4), sharey=True)\n",
    "dfs = []\n",
    "for i, (train_index, test_index) in enumerate(kf.split(iris_df)):\n",
    "    X_train = iris_df.iloc[train_index].loc[:, features]\n",
    "    X_test = iris_df.iloc[test_index].loc[:, features]\n",
    "    y_train = iris_df.iloc[train_index].loc[:, \"target\"]\n",
    "    y_test = iris_df.loc[test_index].loc[:, \"target\"]\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)  # Training the model\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # how many occurances appear in the train set\n",
    "    s_train = iris_df.iloc[train_index].loc[:, \"target_name\"].value_counts()\n",
    "    s_train.name = f\"train {i}\"\n",
    "    s_test = iris_df.iloc[test_index].loc[:, \"target_name\"].value_counts()\n",
    "    s_test.name = f\"test {i}\"\n",
    "    df = pd.concat([s_train, s_test], axis=1, sort=False)\n",
    "    df[\"|\"] = \"|\"\n",
    "    dfs.append(df)\n",
    "\n",
    "    ax[i].scatter(\n",
    "        x=y_train.index,\n",
    "        y=iris_df.iloc[train_index].loc[:, \"target_name\"],\n",
    "        label=\"train\",\n",
    "    )\n",
    "    ax[i].scatter(\n",
    "        x=y_test.index,\n",
    "        y=iris_df.iloc[test_index].loc[:, \"target_name\"],\n",
    "        label=\"test\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    ax[i].set_title(f\"Fold {i+1} accuracy:  {accuracy_score(y_test, y_pred)}\")\n",
    "    ax[i].set_xlabel(\"indices\")\n",
    "\n",
    "ax[0].legend()\n",
    "plt.suptitle(\"Shuffled K-Fold\")\n",
    "plt.show()\n",
    "\n",
    "print(\"In each fold, both training and test sets now contain samples of all 3 species:\")\n",
    "pd.concat(dfs, axis=1, sort=False).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to preserve the initial 1/3 distribution of species for the training and test sets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified K-Fold\n",
    "\n",
    "Stratified K-fold generates train/test splits for each fold such that the distribution of classes is preserved in each group.\n",
    "\n",
    "We use the `StratifiedKFold` function and set `y` to the feature whose distribution we want to preserve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=123)  # shuffled 3-fold\n",
    "\n",
    "# ====================================================================================\n",
    "fig, ax = plt.subplots(1, 3, figsize=(13, 4), sharey=True)\n",
    "dfs = []\n",
    "for i, (train_index, test_index) in enumerate(\n",
    "    kf.split(iris_df, y=iris_df[\"target\"])\n",
    "):  # set y = target here\n",
    "    X_train = iris_df.iloc[train_index].loc[:, features]\n",
    "    X_test = iris_df.iloc[test_index].loc[:, features]\n",
    "    y_train = iris_df.iloc[train_index].loc[:, \"target\"]\n",
    "    y_test = iris_df.loc[test_index].loc[:, \"target\"]\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)  # Training the model\n",
    "\n",
    "    # how many occurances appear in the train set\n",
    "    s_train = iris_df.iloc[train_index].loc[:, \"target_name\"].value_counts()\n",
    "    s_train.name = f\"train {i}\"\n",
    "    s_test = iris_df.iloc[test_index].loc[:, \"target_name\"].value_counts()\n",
    "    s_test.name = f\"test {i}\"\n",
    "    df = pd.concat([s_train, s_test], axis=1, sort=False)\n",
    "    df[\"|\"] = \"|\"\n",
    "    dfs.append(df)\n",
    "\n",
    "    ax[i].scatter(\n",
    "        x=y_train.index,\n",
    "        y=iris_df.iloc[train_index].loc[:, \"target_name\"],\n",
    "        label=\"train\",\n",
    "    )\n",
    "    ax[i].scatter(\n",
    "        x=y_test.index,\n",
    "        y=iris_df.iloc[test_index].loc[:, \"target_name\"],\n",
    "        label=\"test\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    ax[i].set_title(\n",
    "        f\"Fold {i+1} accuracy:  {accuracy_score(y_test, model.predict(X_test))}\"\n",
    "    )\n",
    "    ax[i].set_xlabel(\"indices\")\n",
    "\n",
    "ax[0].legend()\n",
    "plt.suptitle(\"Stratified KFold\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Distribution of 3 species should each be around 1/3 in all groups:\")\n",
    "pd.concat(dfs, axis=1, sort=False).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipping steps\n",
    "\n",
    "You can also skip these intermediate steps and directly obtain the cross-validation accuracies using `cross_validate`\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, score in enumerate(cross_validate(model, X, y, cv=3)[\"test_score\"]):\n",
    "    print(f\"Accuracy for the fold no. {i} on the test set: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or `cross_val_score`\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, score in enumerate(cross_val_score(model, X, y, cv=3)):\n",
    "    print(f\"Accuracy for the fold no. {i} on the test set: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified K-fold on the Titanic dataset\n",
    "\n",
    "Each row in the Titanic dataset represents one person and their attributes -- age, passenger-class, sex, fare they paid, whether they survived. More: https://www.kaggle.com/competitions/titanic/data.\n",
    "\n",
    "Let's see how the features are distributed in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "titanic = sns.load_dataset(\"titanic\").sort_values(by=\"sex\").reset_index(drop=True)\n",
    "\n",
    "print(\"Overview:\")\n",
    "titanic.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply stratified k-fold with K=3 and no shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3\n",
    "kf = StratifiedKFold(n_splits=K, shuffle=False)  # no shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratio of Survivors and Victims in both the training and test sets should remain the same for all folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_counts = []\n",
    "fig, ax = plt.subplots(1, 3, figsize=(13, 4), sharey=True)\n",
    "\n",
    "for k, (train_index, test_index) in enumerate(kf.split(titanic, y=titanic[\"survived\"])):\n",
    "    foldcol = \"fold{}\".format(k + 1)\n",
    "\n",
    "    # train/test splits\n",
    "    titanic[foldcol] = \"train\"\n",
    "    titanic[foldcol][test_index] = \"test\"\n",
    "\n",
    "    # plot\n",
    "    sns.countplot(\n",
    "        x=foldcol,\n",
    "        hue=\"survived\",\n",
    "        order=titanic[foldcol].value_counts().index,\n",
    "        data=titanic,\n",
    "        ax=ax[k],\n",
    "    )\n",
    "    ax[k].set_title(f\"Fold {k+1}\")\n",
    "    ax[k].set_xlabel(\"\")\n",
    "\n",
    "    # count values\n",
    "    fold = titanic.groupby([\"survived\", foldcol]).size().unstack(fill_value=0)\n",
    "    fold.columns = [col + str(k + 1) for col in fold.columns]\n",
    "\n",
    "    death_counts.append(fold)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "pd.concat(death_counts, axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we check the distribution of a different feature, such as sex, it might not be well-balanced for all groups. This imbalance can affect model performance.\n",
    "\n",
    "(we kind of cheated by sorting the initial data by sex to force these results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_counts = []\n",
    "fig, ax = plt.subplots(1, 3, figsize=(13, 4), sharey=True)\n",
    "\n",
    "for k in range(K):\n",
    "    foldcol = \"fold{}\".format(k + 1)\n",
    "\n",
    "    # plot\n",
    "    sns.countplot(\n",
    "        x=foldcol,\n",
    "        hue=\"sex\",\n",
    "        order=titanic[foldcol].value_counts().index,\n",
    "        data=titanic,\n",
    "        ax=ax[k],\n",
    "    )\n",
    "    ax[k].set_title(f\"Fold {k+1}\")\n",
    "    ax[k].set_xlabel(\"\")\n",
    "\n",
    "    # count values\n",
    "    fold = titanic.groupby([foldcol, \"sex\"]).size().unstack(fill_value=0)\n",
    "    fold.columns = [col + str(k + 1) for col in fold.columns]\n",
    "    death_counts.append(fold)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "pd.concat(death_counts, axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, incorporating random sampling (shuffling) can mitigate this imbalance.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3\n",
    "kf = StratifiedKFold(n_splits=K, shuffle=True, random_state=123)  # add shuffling\n",
    "\n",
    "death_counts = []\n",
    "fig, ax = plt.subplots(1, 3, figsize=(13, 4), sharey=True)\n",
    "\n",
    "for k, (train_index, test_index) in enumerate(kf.split(titanic, y=titanic[\"survived\"])):\n",
    "    foldcol = \"fold{}\".format(k + 1)\n",
    "\n",
    "    # train/test splits\n",
    "    titanic[foldcol] = \"train\"\n",
    "    titanic[foldcol][test_index] = \"test\"\n",
    "\n",
    "    # plot\n",
    "    sns.countplot(\n",
    "        x=foldcol,\n",
    "        hue=\"sex\",\n",
    "        order=titanic[foldcol].value_counts().index,\n",
    "        data=titanic,\n",
    "        ax=ax[k],\n",
    "    )\n",
    "    ax[k].set_title(f\"Fold {k+1}\")\n",
    "    ax[k].set_xlabel(\"\")\n",
    "\n",
    "    # count values\n",
    "    fold = titanic.groupby([foldcol, \"sex\"]).size().unstack(fill_value=0)\n",
    "    fold.columns = [col + str(k + 1) for col in fold.columns]\n",
    "    death_counts.append(fold)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "pd.concat(death_counts, axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, balancing the distribution of features can be a complex task, especially if the number of features and classes are large, and should be something to be aware of. We should always check the performance in each fold to get a sense if something has gone wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Cross-validation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeated KFold\n",
    "\n",
    "Repeats K-Fold n times with different randomization in each repetition.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedKFold.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "kf3_shuffled42 = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "kf3_rshuffled42 = RepeatedKFold(n_splits=3, n_repeats=2, random_state=42)\n",
    "\n",
    "# ====================================================================================\n",
    "plt.figure(2, figsize=(12, 4))\n",
    "\n",
    "ax = plt.subplot(121)\n",
    "plot_kfold(ax, kf3_shuffled42.split(fakedata), N_fake)\n",
    "ax.set_title(\"Shuffled 3-Fold with random state 42\")\n",
    "plt.yticks(np.arange(1, 7, 1))\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "plot_kfold(ax, kf3_rshuffled42.split(fakedata), N_fake)\n",
    "ax.set_title(\"Repeated 3-Fold with random state 42\")\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ShuffleSplit\n",
    "\n",
    "Yields indices to split data into training and test sets for K folds but does not guarantee that each sample appears once in the test set.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/cross_validation.html#random-permutations-cross-validation-a-k-a-shuffle-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "kf3_shufflesplit42 = ShuffleSplit(n_splits=3, test_size=0.5, random_state=42)\n",
    "\n",
    "# ====================================================================================\n",
    "plt.figure(2, figsize=(12, 4))\n",
    "\n",
    "ax = plt.subplot(121)\n",
    "plot_kfold(ax, kf3_shuffled42.split(fakedata), N_fake)\n",
    "ax.set_title(\"Shuffled 3-Fold with random state 42\")\n",
    "plt.yticks(np.arange(1, 4, 1))\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "plot_kfold(ax, kf3_shufflesplit42.split(fakedata), N_fake)\n",
    "ax.set_title(\"Shuffle split 3-Fold with random state 42\")\n",
    "plt.yticks(np.arange(1, 4, 1))\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave One Out\n",
    "\n",
    "Leave One Out is the case where each fold leaves one observation out as the test set (i.e. K=N).\n",
    "\n",
    "https://scikit-learn.org/stable/modules/cross_validation.html#leave-one-out-loo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# ====================================================================================\n",
    "plt.figure(2, figsize=(12, 4))\n",
    "\n",
    "ax = plt.subplot(121)\n",
    "plot_kfold(ax, kf3_shuffled42.split(fakedata), N_fake)\n",
    "ax.set_title(\"Shuffled 3-Fold with random state 42\")\n",
    "plt.yticks(np.arange(1, 4, 1))\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "plot_kfold(ax, loo.split(fakedata), N_fake)\n",
    "ax.set_title(\"Leave-one-out\")\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave P Out\n",
    "\n",
    "Leave P out creates ${n \\choose p}$ combination. In our example, if we set p=2, we have: ${25 \\choose 2} = 300$ \n",
    "\n",
    "https://scikit-learn.org/stable/modules/cross_validation.html#leave-p-out-lpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeavePOut\n",
    "\n",
    "lpo = LeavePOut(p=2)\n",
    "\n",
    "# ====================================================================================\n",
    "plt.figure(2, figsize=(12, 4))\n",
    "\n",
    "ax = plt.subplot(121)\n",
    "plot_kfold(ax, kf3_shuffled42.split(fakedata), N_fake)\n",
    "ax.set_title(\"Shuffled 3-Fold with random state 42\")\n",
    "plt.yticks(np.arange(1, 4, 1))\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "plot_kfold(ax, lpo.split(fakedata), N_fake)\n",
    "ax.set_title(\"Leave-P-out (p = 2)\")\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "* Original notebook: https://github.com/vaasha/Machine-leaning-in-examples/blob/master/sklearn/cross-validation/Cross%20Validation.ipynb\n",
    "* Scikit learn: https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "* Wikipedia: https://en.wikipedia.org/wiki/Cross-validation_(statistics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
